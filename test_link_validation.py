#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„é“¾æ¥éªŒè¯æµ‹è¯•
"""

import re
from urllib.parse import urlparse

def is_valid_feishu_url(url):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é£ä¹¦æ–‡æ¡£URL"""
    try:
        parsed = urlparse(url)
        valid_domains = [
            'feishu.cn',
            'larksuite.com', 
            'docs.feishu.cn',
            'bytedance.com'
        ]
        
        return any(domain in parsed.netloc for domain in valid_domains) and (
            '/docx/' in url or 
            '/wiki/' in url or 
            '/docs/' in url or
            '/doc/' in url
        )
    except:
        return False

def test_link():
    """æµ‹è¯•é“¾æ¥"""
    test_url = "https://uvw8s43wky3.feishu.cn/wiki/X9OAwWHJViGlyMkr5LrcZdZZndg"
    
    print("ğŸ” æµ‹è¯•é£ä¹¦é“¾æ¥éªŒè¯")
    print("=" * 50)
    print(f"æµ‹è¯•é“¾æ¥: {test_url}")
    
    # éªŒè¯é“¾æ¥
    is_valid = is_valid_feishu_url(test_url)
    print(f"é“¾æ¥æœ‰æ•ˆæ€§: {'âœ… æœ‰æ•ˆ' if is_valid else 'âŒ æ— æ•ˆ'}")
    
    # è§£æé“¾æ¥ä¿¡æ¯
    parsed = urlparse(test_url)
    print(f"åŸŸå: {parsed.netloc}")
    print(f"è·¯å¾„: {parsed.path}")
    
    # åˆ¤æ–­æ–‡æ¡£ç±»å‹
    if '/wiki/' in test_url:
        doc_type = "Wikiæ–‡æ¡£"
    elif '/docx/' in test_url:
        doc_type = "Docxæ–‡æ¡£"
    else:
        doc_type = "å…¶ä»–æ–‡æ¡£"
    
    print(f"æ–‡æ¡£ç±»å‹: {doc_type}")
    
    # æµ‹è¯•é“¾æ¥è¯»å–
    try:
        with open('feishu_links.txt', 'r', encoding='utf-8') as f:
            links = [line.strip() for line in f.readlines() if line.strip()]
        print(f"âœ… æˆåŠŸä»æ–‡ä»¶è¯»å– {len(links)} ä¸ªé“¾æ¥")
        for i, link in enumerate(links, 1):
            print(f"  {i}. {link}")
    except Exception as e:
        print(f"âŒ è¯»å–é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
    
    print("=" * 50)
    print("ğŸ¯ ç»“è®º: è¯¥é“¾æ¥æ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«å¯¼å‡ºå·¥å…·è¯†åˆ«å’Œå¤„ç†")

if __name__ == "__main__":
    test_link()